# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cITWJnQiZ8_O5SImEKQMRVPjd7KmCCy-
"""

!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html

!pip install fastai

!pip install dataclasses

#Import libraries
import fastai
from fastai import *
from fastai.text import *
import pandas as pd
import numpy as np
from functools import partial
import io
import os

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stop_words = stopwords.words('english')

from sklearn.datasets import fetch_20newsgroups
dataset = fetch_20newsgroups(shuffle = True, random_state = 1, remove = ('headers', 'footers', 'quotes'))
documents = dataset.data

from sklearn.model_selection import train_test_split

df = pd.DataFrame({'label': dataset.target, 'text': dataset.data})

df.head

#df[df['label'] == 10]
#converting this into a binary classification problem by only keeping two classes 1 & 10
df = df[df['label'].isin([1,10])]
df = df.reset_index(drop = True)

df['label'].value_counts()

#Retaining only alphabets and removing othere
df['text'] = df['text'].str.replace("[^a-zA-Z]"," ")

#Tokenization
tokenized_doc = df['text'].apply(lambda x: x.split())

#Remove Stopwords
tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item.lower() not in stop_words])

#Detokenized doc
detokenized_doc = []

for i in range(len(df)):
  t = ' '.join(tokenized_doc[i])
  detokenized_doc.append(t)
 
df['text']=detokenized_doc

#df.head
#Creating training and validation data sets
df_train, df_val = train_test_split(df, stratify = df['label'], test_size = 0.4, random_state = 12)

df_train.shape, df_val.shape

#prepare data for language model
data_lm = TextLMDataBunch.from_df(train_df = df_train, valid_df = df_val, path = '')

#classification model data
data_class = TextClasDataBunch.from_df(train_df = df_train, valid_df = df_val, vocab=data_lm.train_ds.vocab, bs = 32, path = '')

learn = language_model_learner(data_lm, pretrained_model= URLs.WT103, drop_mult=0.7)

#learning object with learning rate 1e-2
learn.fit_one_cycle(1, 1e-2)

learn.save_encoder('ft_enc')

#creating classifier with finetuned encoding
learn = text_classifier_learner(data_class, drop_mult=0.7)
learn.load_encoder('ft_enc')

learn.fit_one_cycle(1, 1e-2)

#get Predictions
preds, targets = learn.get_preds()

predictions = np.argmax(preds, axis=1)
pd.crosstab(predictions, targets)